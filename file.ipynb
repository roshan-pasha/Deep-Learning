{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, MaxPooling2D, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array , array_to_img\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/roshanbtech/Deep_Learning/Frenship_detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os , zipfile\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('/home/roshanbtech/Deep_Learning/Frenship_detection/806312_1381964_bundle_archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/home/roshanbtech/Deep_Learning/Frenship_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Img1137.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Img3633.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Img1474.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Img3459.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Img5578.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Filename\n",
       "0  Img1137.jpg\n",
       "1  Img3633.jpg\n",
       "2  Img1474.jpg\n",
       "3  Img3459.jpg\n",
       "4  Img5578.jpg"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/home/roshanbtech/Deep_Learning/Frenship_detection//train_combined/successful-group-happy-friends-on-260nw-408358303.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-2beff2813dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/roshanbtech/Deep_Learning/Frenship_detection/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/train_combined/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2860\u001b[0m     \"\"\"\n\u001b[1;32m   2861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2862\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2863\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad mode %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/home/roshanbtech/Deep_Learning/Frenship_detection//train_combined/successful-group-happy-friends-on-260nw-408358303.jpg'"
     ]
    }
   ],
   "source": [
    "sample=np.random.choice(train.Images)\n",
    "image=Image.open(os.path.join('/home/roshanbtech/Deep_Learning/Frenship_detection/'+'/train_combined/'+ sample))\n",
    "plt.imshow(image)\n",
    "plt.title(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFileList(myDir, format='.jpg'):\n",
    "    fileList = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(myDir, topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith(format):\n",
    "                fullName = os.path.join(root, name)\n",
    "                fileList.append(fullName)\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(key):\n",
    "    out = createFileList('/home/roshanbtech/Deep_Learning/Frenship_detection/data/train/'+ key)\n",
    "    \n",
    "    out = pd.DataFrame(out)\n",
    "    split_data = out[0].str.split(\"/\")\n",
    "    data = split_data.to_list()\n",
    "    data = [i[-1] for i in data]\n",
    "    train = pd.DataFrame(data)\n",
    "    train = train.rename(columns = {0:'Images'})\n",
    "    train['label'] = key.lower()\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adults = create_dataframe('Adults')\n",
    "train_teenagers = create_dataframe('Teenagers')\n",
    "train_toddlers = create_dataframe('Toddlers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0012.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>00000013.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>00000185.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>00000436.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0014.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Images   label\n",
       "352      0012.jpg  adults\n",
       "400  00000013.jpg  adults\n",
       "337  00000185.jpg  adults\n",
       "114  00000436.jpg  adults\n",
       "358      0014.jpg  adults"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_adults.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>00000312.jpg</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>young-people-pointing-you-smiling-41531521.jpg</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>00000304.jpg</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>00000200.jpg</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>00000181.jpg</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Images      label\n",
       "483                                    00000312.jpg  teenagers\n",
       "326  young-people-pointing-you-smiling-41531521.jpg  teenagers\n",
       "417                                    00000304.jpg  teenagers\n",
       "321                                    00000200.jpg  teenagers\n",
       "72                                     00000181.jpg  teenagers"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_teenagers.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>00000526.jpg</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>00000087.jpg</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>00000424.jpg</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>00000062.jpg</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>children-natural-playground-near-smartinsko-la...</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Images     label\n",
       "448                                       00000526.jpg  toddlers\n",
       "39                                        00000087.jpg  toddlers\n",
       "491                                       00000424.jpg  toddlers\n",
       "295                                       00000062.jpg  toddlers\n",
       "389  children-natural-playground-near-smartinsko-la...  toddlers"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_toddlers.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating three datasets\n",
    "train = pd.concat([train_adults,train_teenagers,train_toddlers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1429, 2)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the Images have repeated names for different labels, so we are removing duplicate images and retaining images with label adult\n",
    "train.drop_duplicates(subset=['Images'], keep='first', inplace=True, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1149, 2)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>118118306-portrait-of-asian-college-student-on...</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>children-natural-playground-near-smartinsko-la...</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>33474175-high-school-students-collaborating-on...</td>\n",
       "      <td>teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>group-diverse-kids-having-fun-together-park-gr...</td>\n",
       "      <td>toddlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>00000137.jpg</td>\n",
       "      <td>adults</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Images      label\n",
       "391  118118306-portrait-of-asian-college-student-on...  teenagers\n",
       "159  children-natural-playground-near-smartinsko-la...   toddlers\n",
       "311  33474175-high-school-students-collaborating-on...  teenagers\n",
       "236  group-diverse-kids-having-fun-together-park-gr...   toddlers\n",
       "113                                       00000137.jpg     adults"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,valid_df=train_test_split(train,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 804 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                             rotation_range=20,\n",
    "                             width_shift_range=0.2, \n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             fill_mode='nearest',\n",
    "                            )\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "dataframe=train_df,\n",
    "directory=\"/home/roshanbtech/Deep_Learning/Frenship_detection/data/train/train_combined\",\n",
    "x_col=\"Images\",\n",
    "y_col=\"label\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 345 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    valid_df, \n",
    "    directory=\"/home/roshanbtech/Deep_Learning/Frenship_detection/data/train/train_combined\",\n",
    "x_col=\"Images\",\n",
    "y_col=\"label\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 314 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test,\n",
    "directory=\"/home/roshanbtech/Deep_Learning/Frenship_detection/Test Data\",\n",
    "x_col=\"Filename\",\n",
    "y_col=None,\n",
    "batch_size=1,\n",
    "seed=42,\n",
    "shuffle=False,\n",
    "class_mode=None,\n",
    "target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 40,408,899\n",
      "Trainable params: 40,408,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(vgg_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights before freezing the conv base: 4\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - 632s 2s/step - loss: 0.9200 - categorical_accuracy: 0.5699 - val_loss: 0.8215 - val_categorical_accuracy: 0.6048\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - 632s 2s/step - loss: 0.7570 - categorical_accuracy: 0.6590 - val_loss: 0.7795 - val_categorical_accuracy: 0.5517\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - 633s 2s/step - loss: 0.7096 - categorical_accuracy: 0.6869 - val_loss: 0.8955 - val_categorical_accuracy: 0.5570\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - 631s 2s/step - loss: 0.6504 - categorical_accuracy: 0.7123 - val_loss: 0.9296 - val_categorical_accuracy: 0.5756\n",
      "Epoch 5/5\n",
      "300/300 [==============================] - 633s 2s/step - loss: 0.5805 - categorical_accuracy: 0.7542 - val_loss: 0.7806 - val_categorical_accuracy: 0.6419\n"
     ]
    }
   ],
   "source": [
    "from keras import losses, optimizers, metrics\n",
    "vgg_model.trainable = False\n",
    "print('This is the number of trainable weights before freezing the conv base:', len(model.trainable_weights))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=5e-5), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=[metrics.categorical_accuracy]\n",
    ")\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              steps_per_epoch=300, \n",
    "                              epochs=5, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 25s 79ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred = model.predict_generator(test_generator,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'teenagers',\n",
       " 'toddlers',\n",
       " 'adults',\n",
       " 'toddlers',\n",
       " 'teenagers',\n",
       " 'teenagers',\n",
       " 'toddlers']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('Sample Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Img1137.jpg</td>\n",
       "      <td>Teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Img3633.jpg</td>\n",
       "      <td>Toddler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Img1474.jpg</td>\n",
       "      <td>Adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Img3459.jpg</td>\n",
       "      <td>Teenagers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Img5578.jpg</td>\n",
       "      <td>Toddler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Filename   Category\n",
       "0  Img1137.jpg  Teenagers\n",
       "1  Img3633.jpg    Toddler\n",
       "2  Img1474.jpg     Adults\n",
       "3  Img3459.jpg  Teenagers\n",
       "4  Img5578.jpg    Toddler"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Category'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
